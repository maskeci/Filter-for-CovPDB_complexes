{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os, shutil\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: All complexes will be filtered\n",
    "def filter_complexes():\n",
    "    # Find and list all complexes in folder that in samae location with this script\n",
    "    complex_folder = \"CovPDB_complexes\"\n",
    "    cwd = os.path.join(os.getcwd(), complex_folder)\n",
    "\n",
    "    complexes = [f for f in listdir(cwd)]\n",
    "\n",
    "    # Create new folder in current directory\n",
    "    name_newfolder = \"Resolution_2.5\"\n",
    "    newfolder = os.path.join(os.getcwd(), name_newfolder)\n",
    "\n",
    "    if not os.path.exists(newfolder):\n",
    "        os.makedirs(newfolder)\n",
    "            \n",
    "    # Seaching and filtering\n",
    "    count = 0\n",
    "    all_proteins = [] # Empty list to store protein names as string to use for web scraper\n",
    "\n",
    "    for protein in complexes: # All index in \"complexes\" list are also name of the folder and name of the pdb file\n",
    "        \n",
    "        protein_folder = os.path.join(cwd, protein) # Directory of parent folder\n",
    "        pdb_file = os.path.join(protein_folder, f'{protein}.pdb') # Directory of pdb file\n",
    "        \n",
    "        with open(pdb_file, 'r') as pdb:\n",
    "            for line in pdb:\n",
    "                if 'REMARK   2 RESOLUTION' in line: # Finds resolution line in pdb\n",
    "                    line = line.split()      \n",
    "                    try:\n",
    "                        resolution = float(line[3])     \n",
    "                        if resolution >= 2.5:\n",
    "                            destination_folder = os.path.join(newfolder, protein)\n",
    "                            shutil.copytree(protein_folder, destination_folder)\n",
    "                            print(f'{protein} with the resolution of {resolution} is copied to {destination_folder} ') \n",
    "                            count += 1\n",
    "                            all_proteins.append(protein)\n",
    "                        break  \n",
    "                    except:\n",
    "                        #print(f'No resolution data of {protein}')\n",
    "                        pass\n",
    "\n",
    "    print(f'>>>> {count} folder filtered <<<<')\n",
    "    print(f'STEP 1 COMPLETED')\n",
    "\n",
    "    return all_proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: CovPDB search, PDB id search term as name of complex as string \n",
    "def Find_url(complex_name: str):\n",
    "    try:\n",
    "        # URL of the page\n",
    "        url = 'https://drug-discovery.vm.uni-freiburg.de/covpdb/search/search_type=by_pdb_idsearch_term=' + complex_name\n",
    "\n",
    "        #  Fetch and parse the HTML content \n",
    "        response = requests.get(url)\n",
    "        html_content = response.content\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find the specific table\n",
    "        all_tables = soup.find_all('table')\n",
    "        for table in all_tables:\n",
    "            thead = table.find('thead')\n",
    "            if thead and 'Complex(es)' in thead.get_text(): # This is the table we are looking for\n",
    "                target_table = table\n",
    "                break\n",
    "\n",
    "        # Extract URLs from the \"SHOW\" links\n",
    "        show_urls = [link['href'] for link in target_table.select('tr.color1 a[href*=\"complex_card\"]')]\n",
    "        url = \"https://drug-discovery.vm.uni-freiburg.de\" + show_urls[0] \n",
    "        \n",
    "        print('STEP 2 COMPLETED')\n",
    "        return url\n",
    "    except:\n",
    "        print(f'STEP 2 ERROR: Certain PDB ID cannot be searched on CovPDB database.')\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Wildcard of the certain complex will be scraped \n",
    "def Data_Scraper(url : str):\n",
    "    try:\n",
    "        # Fetch and parse the HTML content \n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find the specific table\n",
    "        all_tables = soup.find_all('table')\n",
    "        for table in all_tables:\n",
    "            thead = table.find('thead')\n",
    "            if thead and 'Covalent Mechanism' in thead.get_text(): # This is the table we are looking for\n",
    "\n",
    "                target_table = table\n",
    "                break\n",
    "        \n",
    "        # Converting as data frame\n",
    "        rows = table.find_all('tr')\n",
    "        headers = table.find_all('thead')[1].find_all('td')\n",
    "        columns = [header.get_text(strip=True) for header in headers]\n",
    "\n",
    "        data = [] # Empty list to store data\n",
    "\n",
    "        # Iterate through rows and extract data\n",
    "        for row in rows:\n",
    "            cells = row.find_all(['td', 'th'])\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "            data.append(row_data)\n",
    "\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        df = df.iloc[[0]]\n",
    "        df = df.drop(columns='Warhead Structure', errors='ignore')\n",
    "\n",
    "        print('STEP 3 COMPLETED')\n",
    "        return df\n",
    "\n",
    "    except:\n",
    "        print('STEP 3 ERROR: Data cannot be scraped.')\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter and Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Data will be concentrated as dataframe\n",
    "dataset = pd.DataFrame()\n",
    "not_collected = []\n",
    "\n",
    "for protein in all_proteins:\n",
    "    print(f'{protein} data is collecting')\n",
    "    try:\n",
    "        proteindata_url = Find_url(protein)\n",
    "        data = Data_Scraper(proteindata_url)\n",
    "\n",
    "        data = data.rename(index={0: protein})\n",
    "\n",
    "        frames = [dataset,data]\n",
    "        dataset = pd.concat(frames)\n",
    "\n",
    "        print(f'Collected\\n')\n",
    "    except:\n",
    "        not_collected.append(protein)\n",
    "        print(f'{protein} data cannot be collected')\n",
    "\n",
    "print(f'PROCESS FINISHED')\n",
    "print('Not collected proteins: ', not_collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('Dataset.csv')\n",
    "dataset.to_excel('Dataset.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the count of specific values in each column\n",
    "values_to_plot = ['Reaction Name','Warhead Name']\n",
    "def plot_value_counts(df, values_to_plot):\n",
    "    for column in df.columns:\n",
    "        if column in values_to_plot:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            df[column].value_counts().sort_index().plot(kind='bar')\n",
    "            plt.title(f'Distribution of {column}')\n",
    "            plt.xlabel(column)\n",
    "            plt.ylabel('Count')\n",
    "            plt.show()\n",
    "\n",
    "plot_value_counts(dataset, values_to_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
